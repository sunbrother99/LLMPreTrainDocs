# qwen3æŠ€æœ¯æŠ¥å‘Š

Qwen3-235B-A22B åœ¨ä»£ç ã€æ•°å­¦ã€é€šç”¨èƒ½åŠ›ç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸ DeepSeek-R1ã€o1ã€o3-miniã€Grok-3 å’Œ Gemini-2.5-Pro ç­‰é¡¶çº§æ¨¡å‹ç›¸æ¯”ï¼Œè¡¨ç°å‡ºæå…·ç«äº‰åŠ›çš„ç»“æœã€‚æ­¤å¤–ï¼Œå°å‹ MoE æ¨¡å‹ Qwen3-30B-A3B çš„æ¿€æ´»å‚æ•°æ•°é‡æ˜¯ QwQ-32B çš„ 10%ï¼Œè¡¨ç°æ›´èƒœä¸€ç­¹ï¼Œç”šè‡³åƒ Qwen3-4B è¿™æ ·çš„å°æ¨¡å‹ä¹Ÿèƒ½åŒ¹æ•Œ Qwen2.5-72B-Instruct çš„æ€§èƒ½ã€‚

æ­¤æ¬¡å¼€æºåŒ…æ‹¬

ä¸¤æ¬¾MoEæ¨¡å‹ï¼šQwen3-235B-A22Bï¼ˆ2350å¤šäº¿æ€»å‚æ•°ã€ 220å¤šäº¿æ¿€æ´»å‚ï¼‰ï¼Œä»¥åŠQwen3-30B-A3Bï¼ˆ300äº¿æ€»å‚æ•°ã€30äº¿æ¿€æ´»å‚æ•°ï¼‰ï¼›

å…­ä¸ªDenseæ¨¡å‹ï¼šQwen3-32Bã€Qwen3-14Bã€Qwen3-8Bã€Qwen3-4Bã€Qwen3-1.7Bå’ŒQwen3-0.6Bã€‚

Denseæ¨¡å‹çš„å‚æ•°ï¼š

![image](https://github.com/user-attachments/assets/d68c3859-7d65-4c2b-a449-150887814524)

MoEæ¨¡å‹çš„å‚æ•°ï¼š

![image](https://github.com/user-attachments/assets/bf083afc-7dd9-483b-922b-ca4b9d48063c)


Qwen3 MoEæ¨¡å‹æ²¿ç”¨äº†Qwen2.5-MoEçš„ç»†ç²’åº¦ä¸“å®¶åˆ†å‰²å®ç° ã€‚Qwen3 MoEæ¨¡å‹å…±æœ‰128ä¸ªä¸“å®¶ï¼Œæ¯tokenæ¿€æ´»8ä¸ªä¸“å®¶ ã€‚ä¸Qwen2.5-MoEä¸åŒçš„æ˜¯ï¼ŒQwen3-MoEè®¾è®¡ä¸åŒ…å«å…±äº«ä¸“å®¶ ï¼Œé€šè¿‡å…¨å±€è´Ÿè½½å‡è¡¡æŸå¤±å¼ºåˆ¶ä¸“å®¶ä¸“ä¸šåŒ–ï¼Œæå‡ä»»åŠ¡ç‰¹å®šæ€§èƒ½ã€‚è¿™äº›æ¶æ„å’Œè®­ç»ƒä¸Šçš„åˆ›æ–°æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ ã€‚

## åˆ†è¯æ–¹å¼

è¯å…¸ï¼šQwen3æ¨¡å‹ä½¿ç”¨äº†Qwençš„tokenizerï¼Œè¯¥tokenizerå®ç°äº†å­—èŠ‚çº§å­—èŠ‚å¯¹ç¼–ç ï¼ˆBBPEï¼‰ï¼Œè¯æ±‡é‡å¤§å°ä¸º151,669 ã€‚

```text
[åŸå§‹æ–‡æœ¬]  ä½ å¥½
   â†“
[UTF-8 å­—èŠ‚åºåˆ—åŒ–]  [228, 189, 160, 229, 165, 189] #16è¿›åˆ¶è¡¨ç¤º
   â†“
[åˆå§‹ symbol è¡¨è¾¾ï¼šæ¯ä¸ªå­—èŠ‚ä¸ºä¸€ä¸ª symbol]  ['e4', 'bd', 'a0', 'e5', 'a5', 'bd']
   â†“
[æ ¹æ® merges.txt è§„åˆ™è¿›è¡Œ symbol åˆå¹¶]  ['e4bda0', 'e5a5bd']
   â†“
[å¾—åˆ°æœ€ç»ˆ token åºåˆ—ï¼ˆsubwordï¼‰]  ['ä½ ','å¥½']
   â†“
[æŸ¥ vocab.json å¾—åˆ° token_id åºåˆ—]  [19489, 22459]
```

---

## Step-by-Stepï¼šè¯¦ç»†æµç¨‹æ‹†è§£

### Step 0ï¼šåŠ è½½ vocab.json å’Œ merges.txt

#### vocab.json

```json
{
  "ä½ ": 19489,
  "å¥½": 22459,
  "world": 3293,
  "!": 38,
  ...
}
```

è¿™æ˜¯ token â†’ id çš„å­—å…¸ï¼ˆæœ€ç»ˆè¾“å‡ºçš„æ˜¯ idï¼‰

#### merges.txtï¼ˆå¸¦é¢‘ç‡æ’åºçš„åˆå¹¶è§„åˆ™ï¼‰

```txt
# version: 0.2
e4 bd
bd a0
e4bd a0
e5 a5
...
```

è¿™è¡¨ç¤º **symbol åˆå¹¶è§„åˆ™**ï¼Œå³å“ªä¸¤ä¸ª symbol å¦‚æœç›¸é‚»ï¼Œåˆ™åˆå¹¶æˆä¸€ä¸ªæ›´é«˜é¢‘çš„ symbolã€‚

---

### Step 1ï¼šè¾“å…¥å­—ç¬¦ä¸² UTF-8 ç¼–ç ï¼ˆå­—èŠ‚è¡¨ç¤ºï¼‰

```python
text = "ä½ å¥½"
utf8_bytes = text.encode('utf-8')
print(list(utf8_bytes))
# è¾“å‡ºï¼š[228, 189, 160, 229, 165, 189]
# å³ï¼š\xe4\xbd\xa0\xe5\xa5\xbd
```

### Step 2ï¼šåˆå§‹ symbol è¡¨è¾¾ï¼ˆByte-Level Tokenizationï¼‰

å°† UTF-8 æ¯ä¸ªå­—èŠ‚è½¬ä¸º symbol å­—ç¬¦ä¸²ï¼š

```python
['\xe4', '\xbd', '\xa0', '\xe5', '\xa5', '\xbd']
```

æˆ–è€…æŒ‰åå…­è¿›åˆ¶ç®€å†™ä¸ºï¼š

```plaintext
['e4', 'bd', 'a0', 'e5', 'a5', 'bd']
```

æ­¤æ—¶æ¯ä¸ªå­—èŠ‚æ˜¯ä¸€ä¸ªâ€œsymbolâ€å•ä½ã€‚

---

### Step 3ï¼šæ ¹æ® merges.txt æ‰§è¡Œ BPE åˆå¹¶

æŒ‰ `merges.txt` ä¸­çš„é¡ºåºï¼Œé‡å¤åˆå¹¶ç¬¦å·å¯¹ï¼š

åˆå§‹çŠ¶æ€ï¼š

```plaintext
['e4', 'bd', 'a0', 'e5', 'a5', 'bd']
```

æ‰§è¡Œåˆå¹¶è§„åˆ™ï¼š

1. `e4 + bd â†’ e4bd`
2. `e4bd + a0 â†’ e4bda0`ï¼ˆå³ â€œä½ â€ çš„ UTF-8 å…¨å­—èŠ‚åˆå¹¶ï¼‰
3. `e5 + a5 â†’ e5a5`
4. `e5a5 + bd â†’ e5a5bd`ï¼ˆå³ â€œå¥½â€ çš„ UTF-8 å…¨å­—èŠ‚åˆå¹¶ï¼‰

å¾—åˆ°ï¼š

```plaintext
['e4bda0', 'e5a5bd']
```

è¿™äº›åˆå¹¶å¥½çš„ symbol ç§°ä¸º **æœ€ç»ˆ subwords**ã€‚

---

### Step 4ï¼šä» vocab.json æ˜ å°„ä¸º token id

token â†’ idï¼š

```json
{
  "ä½ "      â†’ 19489,
  "å¥½"      â†’ 22459
}
```

åœ¨ vocab ä¸­ï¼Œ'ä½ ' çš„ UTF-8 åˆå¹¶å­—ç¬¦ä¸² `e4bda0` è¢«æ˜ å°„ä¸º token `'ä½ '`ï¼Œå¹¶å¯¹åº”æŸä¸ª IDã€‚

æœ€ç»ˆè¾“å‡º token-id åºåˆ—ï¼š

```python
[19489, 22459]
```

---

## utf-8ç¼–ç åçš„symbolå¦‚ä½•é€šè¿‡BPEçš„æ–¹å¼åˆå¹¶ä¸ºtoken(subword)çš„ï¼Ÿ

éå¸¸å¥½ï¼Œè¿™ä¸ªé—®é¢˜æ˜¯ç†è§£ BPEï¼ˆåŒ…æ‹¬ Qwen çš„ BBPEï¼‰ç¼–ç çš„å…³é”®ã€‚æˆ‘ä»¬ä¸‹é¢å°†é€šè¿‡**å…·ä½“æœºåˆ¶ + ä¸¾ä¾‹ + ä¼ªä»£ç **æ¥å®Œæ•´è§£é‡Š **ç¬¬ä¸‰æ­¥ merges.txt çš„åˆå¹¶è¿‡ç¨‹**ã€‚

---

## èƒŒæ™¯é—®é¢˜ç®€åŒ–é‡è¿°ï¼š

ä½ å·²ç»å¾—åˆ°äº†è¾“å…¥å¥å­çš„ UTF-8 å­—èŠ‚åºåˆ—ï¼Œä¾‹å¦‚ï¼š

```plaintext
['e4', 'bd', 'a0', 'e5', 'a5', 'bd']  # â€œä½ å¥½â€
```

ç°åœ¨æˆ‘ä»¬è¦æŒ‰ merges.txt æ¥åˆå¹¶è¿™ä¸² symbolï¼Œå˜æˆï¼š

```plaintext
['e4bda0', 'e5a5bd']  # æœ€ç»ˆ token
```

**é—®é¢˜æ˜¯ï¼šæ€ä¹ˆåˆå¹¶ï¼Ÿæ˜¯æ€ä¹ˆç”¨ merges.txt åˆå¹¶è¿™ä¸² symbol çš„ï¼Ÿæ˜¯ä¸æ˜¯é€è¡Œéå†ï¼Ÿ**

---

## ç­”æ¡ˆæ¦‚è¦ï¼š

 > **åˆå¹¶è¿‡ç¨‹æ˜¯åŸºäº merges.txt å®šä¹‰çš„ symbol å¯¹åˆå¹¶ä¼˜å…ˆçº§ï¼Œè¿­ä»£æ‰§è¡Œæœ€é«˜ä¼˜å…ˆçº§çš„åˆå¹¶å¯¹ï¼Œç›´åˆ°æ— æ³•å†åˆå¹¶ä¸ºæ­¢ã€‚**

---

## Step-by-Step ä¸¾ä¾‹è¯´æ˜

### å‡†å¤‡ææ–™ï¼š

#### åˆå§‹ symbol åºåˆ—ï¼ˆUTF-8 byte åºåˆ—ï¼‰ï¼š

```python
symbols = ['e4', 'bd', 'a0', 'e5', 'a5', 'bd']
```

#### merges.txtï¼ˆå‰å‡ è¡Œï¼‰ï¼š

```txt
# version: 0.2
e4 bd       â†’ åˆå¹¶ä¸º e4bd
bd a0       â†’ åˆå¹¶ä¸º bda0
e4bd a0     â†’ åˆå¹¶ä¸º e4bda0
e5 a5       â†’ åˆå¹¶ä¸º e5a5
a5 bd       â†’ åˆå¹¶ä¸º a5bd
e5a5 bd     â†’ åˆå¹¶ä¸º e5a5bd
```

### ğŸ¯ ç›®æ ‡ï¼šå¯¹ symbol åˆ—è¡¨è¿›è¡Œè¿­ä»£åˆå¹¶ï¼Œç›´åˆ°æ²¡æœ‰å¯ä»¥åˆå¹¶çš„å¯¹ä¸ºæ­¢ã€‚

---

## åˆå¹¶ç®—æ³•åŸç†ï¼ˆç»å…¸ BPE åˆå¹¶ï¼‰

æ¯ä¸€è½®æ“ä½œå¦‚ä¸‹ï¼š

1. éå†å½“å‰ symbol åˆ—è¡¨ä¸­çš„ **æ‰€æœ‰ç›¸é‚»äºŒå…ƒå¯¹ï¼ˆbigramsï¼‰**ï¼›
2. åœ¨ merges.txt ä¸­æ‰¾åˆ°è¿™äº›å¯¹ä¸­ä¼˜å…ˆçº§æœ€é«˜çš„ï¼ˆå³æœ€é å‰çš„ï¼‰ï¼›
3. åˆå¹¶è¿™ä¸ªå¯¹ï¼›
4. å›åˆ°ç¬¬ 1 æ­¥ï¼Œç›´åˆ°ä¸èƒ½å†åˆå¹¶ã€‚

---

## ä¸¾ä¾‹è¿‡ç¨‹ï¼šä»¥â€œä½ å¥½â€ä¸ºä¾‹

åˆå§‹ symbolsï¼š

```plaintext
['e4', 'bd', 'a0', 'e5', 'a5', 'bd']
```

### ç¬¬ä¸€æ¬¡éå†ç›¸é‚»å¯¹ï¼š

```plaintext
('e4', 'bd') âœ… åœ¨ merges.txtï¼Œä¸”ä¼˜å…ˆçº§æœ€é«˜
('bd', 'a0') âœ… å­˜åœ¨ï¼Œä½†åé¢ä¸€ç‚¹
('a0', 'e5')
('e5', 'a5') âœ… å­˜åœ¨
('a5', 'bd') âœ… å­˜åœ¨
```

åˆå¹¶ `'e4' + 'bd'` â†’ `'e4bd'`ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰

æ›´æ–°åºåˆ—ï¼š

```plaintext
['e4bd', 'a0', 'e5', 'a5', 'bd']
```

---

### ç¬¬äºŒè½®éå†ç›¸é‚»å¯¹ï¼š

```plaintext
('e4bd', 'a0') âœ… å­˜åœ¨ â†’ åˆå¹¶
('a0', 'e5')
('e5', 'a5') âœ…
('a5', 'bd') âœ…
```

åˆå¹¶ `'e4bd' + 'a0'` â†’ `'e4bda0'`

æ›´æ–°ï¼š

```plaintext
['e4bda0', 'e5', 'a5', 'bd']
```

---

### ç¬¬ä¸‰è½®éå†ï¼š

```plaintext
('e4bda0', 'e5') âŒ ä¸åœ¨ merges
('e5', 'a5') âœ…
('a5', 'bd') âœ…
```

åˆå¹¶ `'e5' + 'a5'` â†’ `'e5a5'`

æ›´æ–°ï¼š

```plaintext
['e4bda0', 'e5a5', 'bd']
```

---

### ç¬¬å››è½®éå†ï¼š

```plaintext
('e4bda0', 'e5a5')
('e5a5', 'bd') âœ… â†’ åˆå¹¶ â†’ 'e5a5bd'
```

æœ€ç»ˆåˆå¹¶ï¼š

```plaintext
['e4bda0', 'e5a5bd']  âœ… â€œä½ â€å’Œâ€œå¥½â€
```

---

## âš™ï¸ åˆå¹¶ä¼ªä»£ç ï¼ˆç®€åŒ–ç‰ˆï¼‰

```python
def bpe_merge(symbols, merge_ranks):
    while True:
        # æ‰¾æ‰€æœ‰ç›¸é‚»å¯¹
        pairs = [(symbols[i], symbols[i+1]) for i in range(len(symbols) - 1)]
        # æ‰¾å¯åˆå¹¶çš„å¯¹åŠå…¶ä¼˜å…ˆçº§
        candidates = [(pair, merge_ranks.get(pair)) for pair in pairs if pair in merge_ranks]
        if not candidates:
            break
        # æ‰¾ä¼˜å…ˆçº§æœ€é«˜ï¼ˆrankæœ€å°ï¼‰çš„
        best_pair, _ = min(candidates, key=lambda x: x[1])
        # åˆå¹¶è¯¥å¯¹
        new_symbols = []
        i = 0
        while i < len(symbols):
            if i < len(symbols) - 1 and (symbols[i], symbols[i+1]) == best_pair:
                new_symbols.append(symbols[i] + symbols[i+1])
                i += 2
            else:
                new_symbols.append(symbols[i])
                i += 1
        symbols = new_symbols
    return symbols
```

å…¶ä¸­ `merge_ranks` æ˜¯ä» merges.txt è¯»å–å¹¶æ’åºç”Ÿæˆçš„ pair â†’ rank å­—å…¸ï¼š

```python
merge_ranks = {('e4', 'bd'): 0, ('bd', 'a0'): 1, ...}
```

---

## âœ… æ€»ç»“ä¸€å¥è¯ï¼š

> åˆå¹¶è¿‡ç¨‹æ˜¯ä¸€ä¸ª **æŒ‰ merges.txt é¡ºåºå®šä¹‰ä¼˜å…ˆçº§çš„ã€è¿­ä»£è´ªå¿ƒåˆå¹¶ç›¸é‚» symbol å¯¹çš„è¿‡ç¨‹**ã€‚ä¸æ˜¯æš´åŠ›å…¨éå†ï¼Œè€Œæ˜¯æ™ºèƒ½æŒ‘å‡ºâ€œå½“å‰èƒ½åˆå¹¶çš„æœ€ä¼˜å¯¹â€ã€‚

---




# è®­ç»ƒæ–¹å¼ï¼šé¢„è®­ç»ƒ+åè®­ç»ƒ

## é¢„è®­ç»ƒï¼ˆPre-trainingï¼‰ï¼šæ•°æ®æ„å»ºä¸ä¸‰é˜¶æ®µç­–ç•¥

åœ¨é¢„è®­ç»ƒæ–¹é¢ï¼ŒQwen3 çš„æ•°æ®é›†ç›¸æ¯” Qwen2.5 æœ‰äº†æ˜¾è‘—æ‰©å±•ã€‚Qwen2.5æ˜¯åœ¨ 18 ä¸‡äº¿ä¸ª token ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œè€Œ Qwen3 ä½¿ç”¨çš„æ•°æ®é‡å‡ ä¹æ˜¯å…¶ä¸¤å€ï¼Œè¾¾åˆ°äº†çº¦ __36 ä¸‡äº¿ä¸ª token__ ï¼Œæ¶µç›–äº† 119 ç§è¯­è¨€å’Œæ–¹è¨€ã€‚ä¸ºäº†æ„å»ºè¿™ä¸ªåºå¤§çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬ä¸ä»…ä»ç½‘ç»œä¸Šæ”¶é›†æ•°æ®ï¼Œè¿˜ä» PDF æ–‡æ¡£ä¸­æå–ä¿¡æ¯ã€‚æˆ‘ä»¬ä½¿ç”¨ Qwen2.5-VL ä»è¿™äº›æ–‡æ¡£ä¸­æå–æ–‡æœ¬ï¼Œå¹¶ç”¨ Qwen2.5 æ”¹è¿›æå–å†…å®¹çš„è´¨é‡ã€‚ä¸ºäº†å¢åŠ æ•°å­¦å’Œä»£ç æ•°æ®çš„æ•°é‡ï¼Œæˆ‘ä»¬åˆ©ç”¨ Qwen2.5-Math å’Œ Qwen2.5-Coder è¿™ä¸¤ä¸ªæ•°å­¦å’Œä»£ç é¢†åŸŸçš„ä¸“å®¶æ¨¡å‹åˆæˆæ•°æ®ï¼Œåˆæˆäº†åŒ…æ‹¬æ•™ç§‘ä¹¦ã€é—®ç­”å¯¹ä»¥åŠä»£ç ç‰‡æ®µç­‰å¤šç§å½¢å¼çš„æ•°æ®ã€‚

### Qwen3æ¨¡å‹é‡‡ç”¨ä¸‰é˜¶æ®µé¢„è®­ç»ƒè¿‡ç¨‹ï¼š

__ç¬¬ä¸€é˜¶æ®µï¼šåŸºæœ¬è¯­è¨€çŸ¥è¯†å’Œé€šç”¨èƒ½åŠ›è®­ç»ƒã€‚__ æ‰€æœ‰Qwen3æ¨¡å‹ä½¿ç”¨4,096 tokençš„åºåˆ—é•¿åº¦ï¼Œåœ¨è¶…è¿‡30ä¸‡äº¿tokençš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒ ã€‚æ­¤é˜¶æ®µæ—¨åœ¨å»ºç«‹æ¨¡å‹çš„è¯­è¨€èƒ½åŠ›å’Œé€šç”¨ä¸–ç•ŒçŸ¥è¯†åŸºç¡€ï¼Œè®­ç»ƒæ•°æ®è¦†ç›–119ç§è¯­è¨€å’Œæ–¹è¨€ ã€‚

__ç¬¬äºŒé˜¶æ®µï¼šå¤æ‚æ¨ç†èƒ½åŠ›è®­ç»ƒã€‚__ æ­¤é˜¶æ®µçš„æ¨¡å‹ä½¿ç”¨4,096 tokençš„åºåˆ—é•¿åº¦ï¼Œåœ¨çº¦5ä¸‡äº¿é«˜è´¨é‡tokenä¸Šè¿›è¡Œè¿›ä¸€æ­¥é¢„è®­ç»ƒï¼Œé¢„è®­ç»ƒæ•°æ®åŒ…æ‹¬STEMã€ç¼–ç ã€æ¨ç†ã€æ•°å­¦å’Œåˆæˆæ•°æ®ï¼ˆçŸ¥è¯†å¯†é›†å‹æ•°æ®ï¼‰ç­‰ ã€‚åœ¨æ­¤é˜¶æ®µè¿˜åŠ é€Ÿäº†å­¦ä¹ ç‡è¡°å‡ ã€‚

__ç¬¬ä¸‰é˜¶æ®µï¼šé•¿ä¸Šä¸‹æ–‡èƒ½åŠ›è®­ç»ƒã€‚__ æ­¤é˜¶æ®µå°†æ¨¡å‹4Kçš„è¾“å…¥é•¿åº¦æ‰©å±•åˆ°32Kï¼Œé«˜è´¨é‡é•¿ä¸Šä¸‹æ–‡è¯­æ–™ä¸­ï¼Œ75%çš„æ–‡æœ¬é•¿åº¦åœ¨16,384åˆ°32,768 tokenä¹‹é—´ï¼Œ25%çš„æ–‡æœ¬é•¿åº¦åœ¨4,096åˆ°16,384 tokenä¹‹é—´ ã€‚æŠ¥å‘ŠæåŠæ²¿ç”¨Qwen2.5çš„åšæ³•ï¼Œä½¿ç”¨ABFæŠ€æœ¯å°†RoPEçš„åŸºç¡€é¢‘ç‡ä»10,000æé«˜åˆ°1,000,000 ã€‚åŒæ—¶ï¼Œå¼•å…¥YARNå’ŒDual Chunk Attention (DCA)æŠ€æœ¯ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°åºåˆ—é•¿åº¦å®¹é‡çš„å››å€å¢é•¿ ã€‚

## åè®­ç»ƒï¼ˆPost-trainingï¼‰ï¼šå››é˜¶æ®µèƒ½åŠ›ç²¾ç‚¼ï¼Œå®ç°é€æ­¥æ¨ç†å’Œå¿«é€Ÿå“åº”

<img width="827" alt="image" src="https://github.com/user-attachments/assets/28cc873d-0a43-428c-8d8e-657dd5c3a229" />

ä¸ºäº†å¼€å‘èƒ½å¤ŸåŒæ—¶å…·å¤‡æ€è€ƒæ¨ç†å’Œå¿«é€Ÿå“åº”èƒ½åŠ›çš„æ··åˆæ¨¡å‹ï¼Œæˆ‘ä»¬å®æ–½äº†ä¸€ä¸ªå››é˜¶æ®µçš„è®­ç»ƒæµç¨‹ã€‚è¯¥æµç¨‹åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é•¿æ€ç»´é“¾å†·å¯åŠ¨ï¼Œï¼ˆ2ï¼‰é•¿æ€ç»´é“¾å¼ºåŒ–å­¦ä¹ ï¼Œï¼ˆ3ï¼‰æ€ç»´æ¨¡å¼èåˆï¼Œä»¥åŠï¼ˆ4ï¼‰é€šç”¨å¼ºåŒ–å­¦ä¹ ã€‚

__ç¬¬ä¸€é˜¶æ®µï¼šé•¿æ€ç»´é“¾å†·å¯åŠ¨ã€‚__ ä½¿ç”¨å¤šæ ·çš„é•¿æ€ç»´é“¾æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ˆSFTï¼‰ï¼Œæ¶µç›–äº†æ•°å­¦ã€ä»£ç ã€é€»è¾‘æ¨ç†å’Œ STEM é—®é¢˜ç­‰å¤šç§ä»»åŠ¡å’Œé¢†åŸŸã€‚è¿™ä¸€è¿‡ç¨‹æ—¨åœ¨ä¸ºæ¨¡å‹é…å¤‡åŸºæœ¬çš„æ¨ç†èƒ½åŠ›ã€‚

__ç¬¬äºŒé˜¶æ®µï¼šé•¿æ€ç»´é“¾å¼ºåŒ–å­¦ä¹ ã€‚__ è¿™ä¸€é˜¶æ®µçš„é‡ç‚¹æ˜¯å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±æ¥å¢å¼ºæ¨¡å‹çš„æ¢ç´¢å’Œé’»ç ”èƒ½åŠ›ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›éº»çƒ¦è®©æ¨¡å‹èƒ½å¤Ÿæ›´åŠ æœ‰æ•ˆçš„å¯»æ‰¾æœ€ä½³ç­”æ¡ˆã€‚

__ç¬¬ä¸‰é˜¶æ®µï¼šæ€ç»´æ¨¡å¼èåˆã€‚__ æˆ‘ä»¬åœ¨ä¸€ä»½åŒ…æ‹¬é•¿æ€ç»´é“¾æ•°æ®å’Œå¸¸ç”¨çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®çš„ç»„åˆæ•°æ®ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ˆSFTï¼‰ï¼Œå°†éæ€è€ƒæ¨¡å¼æ•´åˆåˆ°æ€è€ƒæ¨¡å‹ä¸­ã€‚ç¡®ä¿äº†æ¨ç†å’Œå¿«é€Ÿå“åº”èƒ½åŠ›çš„æ— ç¼ç»“åˆã€‚

__ç¬¬å››é˜¶æ®µï¼šé€šç”¨å¼ºåŒ–å­¦ä¹ ã€‚__ æˆ‘ä»¬åœ¨åŒ…æ‹¬æŒ‡ä»¤éµå¾ªã€æ ¼å¼éµå¾ªå’Œ Agent èƒ½åŠ›ç­‰åœ¨å†…çš„ 20 å¤šä¸ªé€šç”¨é¢†åŸŸçš„ä»»åŠ¡ä¸Šåº”ç”¨äº†å¼ºåŒ–å­¦ä¹ ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„é€šç”¨èƒ½åŠ›å¹¶çº æ­£ä¸è‰¯è¡Œä¸ºã€‚

## Qwen3å¦‚ä½•å®ç°æ€è€ƒ/ä¸æ€è€ƒçš„çš„æ§åˆ¶

### ç¡¬å¼€å…³ï¼šenable_thinking=True å¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œenable_thinking=False ç¦ç”¨æ€è€ƒæ¨¡å¼ã€‚

qwen3é€šè¿‡tokenizer.apply_chat_templateçš„enable_thinkingå‚æ•°æ¥å®ç°æ€è€ƒæ¨¡å¼å’Œéæ€è€ƒæ¨¡å¼çš„åˆ‡æ¢ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œqwen3å¯ç”¨äº†æ€è€ƒæ¨¡å¼ã€‚

```python
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=True  # True is the default value for enable_thinking.
)
```

åœ¨enable_thinking=Trueï¼ˆæ€è€ƒæ¨¡å¼ï¼‰ä¸‹ï¼Œæ¨¡å‹ä¼šç”ŸæˆåŒ…è£¹åœ¨\<think\>...\</think\> å—ä¸­çš„æ€è€ƒå†…å®¹ï¼Œç„¶åæ˜¯æœ€ç»ˆå“åº”ã€‚
æ€è€ƒæ¨¡å¼ä¸‹ï¼Œè¾“å…¥ç»™æ¨¡å‹çš„æ¨¡æ¿ä¸º
```text
<|im_start|>system
ä½ æ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚<|im_end|>
<|im_start|>user
ç»™æˆ‘è®²è®²å¤§è¯­è¨€æ¨¡å‹ã€‚<|im_end|>
<|im_start|>assistant
```

```text
å¯¹äºæ€è€ƒæ¨¡å¼ï¼Œå®˜æ–¹æç¤ºè¯·ä½¿ç”¨Temperature=0.6ã€TopP=0.95ã€TopK=20å’ŒMinP=0ï¼ˆ ä¸­çš„é»˜è®¤è®¾ç½®generation_config.jsonï¼‰ã€‚
è¯·å‹¿ä½¿ç”¨è´ªå©ªè§£ç ï¼Œå› ä¸ºå®ƒä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™å’Œæ— ä¼‘æ­¢çš„é‡å¤ã€‚https://huggingface.co/Qwen/Qwen3-32B
```

åœ¨enable_thinking=Falseï¼ˆéæ€è€ƒæ¨¡å¼ï¼‰ä¸‹ï¼Œåœ¨æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹ä¸ä¼šç”Ÿæˆä»»ä½•æ€è€ƒå†…å®¹ã€‚
éæ€è€ƒæ¨¡å¼ä¸‹ï¼Œè¾“å…¥ç»™æ¨¡å‹çš„æ¨¡æ¿ä¸º

```text

<|im_start|>system
ä½ æ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚<|im_end|>
<|im_start|>user
ç»™æˆ‘è®²è®²å¤§è¯­è¨€æ¨¡å‹ã€‚<|im_end|>
<|im_start|>assistant
<think>

</think>

```


```text
å¯¹äºéæ€è€ƒæ¨¡å¼ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨Temperature=0.7ã€TopP=0.8ã€TopK=20å’ŒMinP=0ã€‚
```

**æ³¨æ„ï¼š** æ€è€ƒæ¨¡å¼ä¸‹ï¼Œå¯¹è¯æ¨¡æ¿ä¸æ™®é€šæ¨¡æ¿ç›¸åŒï¼Œæ²¡æœ‰ä»»ä½•å˜åŒ–ã€‚éæ€è€ƒæ¨¡å¼ä¸‹ï¼Œå¯¹è¯æ¨¡æ¿ä¼šåœ¨<|im_start|>assistantÂ çš„åé¢æ·»åŠ ä¸€ä¸ªç©ºçš„Â \<think\>\</think\>ã€‚åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œç”¨æˆ·è¾“å…¥åªä¼šå½±å“åˆ°<|im_start|>user\nç»™æˆ‘è®²è®²å¤§è¯­è¨€æ¨¡å‹ã€‚<|im_end|>Â è¿™ä¸€æ®µï¼Œä»<|im_start|>assistantå¼€å§‹çš„å†…å®¹éƒ½æ˜¯æ¨¡å‹åº”è¯¥è¦ç”Ÿæˆçš„å†…å®¹ï¼Œæ•´ä¸ªQwen3æ§åˆ¶æ··åˆæ€è€ƒåˆ‡æ¢çš„æµç¨‹ä¸ºï¼š

1ã€é¦–å…ˆï¼ŒQwen3 ä¼šé»˜è®¤æ€è€ƒï¼Œä¹Ÿå°±æ˜¯ç”Ÿæˆ \<think\> ... \</think\> çš„å†…å®¹ã€‚

2ã€å¦‚æœæˆ‘ä»¬ä¸æƒ³è®©æ¨¡å‹æ€è€ƒï¼Œæˆ‘ä»¬åªéœ€è¦æå‰â€œæ³¨å…¥â€ä¸€æ®µç©ºç™½çš„æ€è€ƒå†…å®¹ï¼Œä¹Ÿå°±æ˜¯ \<think\>ã€Œç©ºç™½ã€\</think\>ï¼Œè®©æ¨¡å‹è®¤ä¸ºã€Œæ€è€ƒã€è¿™ä¸ªè¿‡ç¨‹å·²ç»ç»“æŸäº†ï¼Œæ¥ä¸‹æ¥éƒ½æ˜¯æ™®é€šå›å¤ã€‚

3ã€è¿™æ ·å°±å®Œæˆäº†æ··åˆæ€è€ƒçš„å¯åœã€‚

### è½¯å¼€å…³ enable_thinking=True å¯ç”¨æ€è€ƒæ¨¡å¼æ—¶ï¼Œé€šè¿‡ç”¨æˆ·è¾“å…¥ï¼Œåœ¨æ€è€ƒæ¨¡å¼å’Œè´¹æ€è€ƒæ¨¡å¼ä¹‹é—´åˆ‡æ¢ã€‚

å®ç°æ–¹å¼ï¼šåœ¨ç”¨æˆ·promptæˆ–system promptä¸­æ·»åŠ /thinkæˆ–/no_think,å®ç°åœ¨ä¸åŒè½®è¾“å…¥ä¹‹é—´åˆ‡æ¢æ¨¡å‹çš„æ€ç»´æ¨¡å¼ã€‚åœ¨å¤šå›åˆå¯¹è¯ä¸­ï¼Œæ¨¡å‹å°†éµå¾ªæœ€åä¸€æ¡çš„æŒ‡ä»¤ã€‚

ä¸ºäº†å®ç°APIå±‚çº§çš„å…¼å®¹ï¼Œå½“enable_thinking=Trueæ—¶ï¼Œæ— è®ºç”¨æˆ·ä½¿ç”¨/thinkè¿˜æ˜¯/no_thinkï¼Œæ¨¡å‹éƒ½ä¼šå§‹ç»ˆè¾“å‡ºä¸€ä¸ªåŒ…è£¹åœ¨\<think\>...\</think\>ä¸­çš„å—ã€‚å½“ç”¨æˆ·è¾“å…¥/thinkæ—¶ï¼Œæ¨¡å‹è¾“å‡ºçš„\<think\>...\</think\>å—ä¸­ä¸ºæ­£å¸¸çš„æ€è€ƒå†…å®¹ï¼Œå½“ç”¨æˆ·è¾“å…¥/no_thinkæ—¶ï¼Œæ¨¡å‹ä»ç„¶è¾“å‡ºåŒ…å«\<think\>\</think\>çš„ç©ºç™½æ€è€ƒå—ã€‚æ­¤æ–¹æ¡ˆä¸ºQwen3çš„ã€Œç©ºç™½æ€è€ƒæ³¨å…¥ã€æ–¹æ¡ˆã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å…ˆæŒ‰ç…§å¦‚ä»Šé€šç”¨è®­ç»ƒæ€è€ƒæ¨¡å‹çš„æ–¹å¼ï¼Œè®­ç»ƒå‡ºä¸€ä¸ªä¼šæ€è€ƒçš„æ¨¡å‹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨è®­ç»ƒä¸­è®¾è®¡è¿™æ ·ä¸€å¥—æ•°æ®ï¼šåŠ äº† /think æç¤ºçš„ï¼Œå¯¹åº”å›å¤å°±æ˜¯æœ‰æ€è€ƒå†…å®¹çš„ï¼›åŠ äº† /no_think æç¤ºçš„ï¼Œå¯¹åº”å›å¤å°±æ˜¯æ€è€ƒå†…å®¹ä¸ºç©ºç™½çš„ã€‚è¿™æ ·æ¨¡å‹å°±èƒ½å¤Ÿå­¦ä¼šå“åº”è½¯æç¤ºäº†ã€‚

æ­£å¦‚Qwen3æŠ€æœ¯æŠ¥å‘Šä¸­çš„åè®­ç»ƒçš„ç¬¬ä¸‰ä¸ªé˜¶æ®µï¼Œå°±æ˜¯ç”¨æ¥è®­ç»ƒæ¨¡å‹æ€è€ƒæ¨¡å¼çš„è½¯æ§åˆ¶çš„ã€‚

## æ€è€ƒã€éæ€è€ƒæ¨¡å¼åˆ‡æ¢ç¤ºä¾‹

```pyton
from transformers import AutoModelForCausalLM, AutoTokenizer

class QwenChatbot:
    def __init__(self, model_name="Qwen3-30B-A3B/Qwen3-30B-A3B"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.history = []

    def generate_response(self, user_input):
        messages = self.history + [{"role": "user", "content": user_input}]

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        inputs = self.tokenizer(text, return_tensors="pt")
        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()
        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)

        # Update history
        self.history.append({"role": "user", "content": user_input})
        self.history.append({"role": "assistant", "content": response})

        return response

# Example Usage
if __name__ == "__main__":
    chatbot = QwenChatbot()

    # First input (without /think or /no_think tags, thinking mode is enabled by default)
    user_input_1 = "How many r's in strawberries?"
    print(f"User: {user_input_1}")
    response_1 = chatbot.generate_response(user_input_1)
    print(f"Bot: {response_1}")
    print("----------------------")

    # Second input with /no_think
    user_input_2 = "Then, how many r's in blueberries? /no_think"
    print(f"User: {user_input_2}")
    response_2 = chatbot.generate_response(user_input_2)
    print(f"Bot: {response_2}") 
    print("----------------------")

    # Third input with /think
    user_input_3 = "Really? /think"
    print(f"User: {user_input_3}")
    response_3 = chatbot.generate_response(user_input_3)
    print(f"Bot: {response_3}")
```






