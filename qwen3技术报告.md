# qwen3技术报告

Qwen3-235B-A22B 在代码、数学、通用能力等基准测试中，与 DeepSeek-R1、o1、o3-mini、Grok-3 和 Gemini-2.5-Pro 等顶级模型相比，表现出极具竞争力的结果。此外，小型 MoE 模型 Qwen3-30B-A3B 的激活参数数量是 QwQ-32B 的 10%，表现更胜一筹，甚至像 Qwen3-4B 这样的小模型也能匹敌 Qwen2.5-72B-Instruct 的性能。

此次开源包括

两款MoE模型：Qwen3-235B-A22B（2350多亿总参数、 220多亿激活参），以及Qwen3-30B-A3B（300亿总参数、30亿激活参数）；

六个Dense模型：Qwen3-32B、Qwen3-14B、Qwen3-8B、Qwen3-4B、Qwen3-1.7B和Qwen3-0.6B。

Dense模型的参数：

![image](https://github.com/user-attachments/assets/d68c3859-7d65-4c2b-a449-150887814524)

MoE模型的参数：

![image](https://github.com/user-attachments/assets/bf083afc-7dd9-483b-922b-ca4b9d48063c)


Qwen3 MoE模型沿用了Qwen2.5-MoE的细粒度专家分割实现 。Qwen3 MoE模型共有128个专家，每token激活8个专家 。与Qwen2.5-MoE不同的是，Qwen3-MoE设计不包含共享专家 ，通过全局负载均衡损失强制专家专业化，提升任务特定性能。这些架构和训练上的创新显著提升了模型在下游任务中的性能 。

词典：Qwen3模型使用了Qwen的tokenizer，该tokenizer实现了字节级字节对编码（BBPE），词汇量大小为151,669 。


# 训练方式：预训练+后训练

## 预训练（Pre-training）：数据构建与三阶段策略

在预训练方面，Qwen3 的数据集相比 Qwen2.5 有了显著扩展。Qwen2.5是在 18 万亿个 token 上进行预训练的，而 Qwen3 使用的数据量几乎是其两倍，达到了约 __36 万亿个 token__ ，涵盖了 119 种语言和方言。为了构建这个庞大的数据集，我们不仅从网络上收集数据，还从 PDF 文档中提取信息。我们使用 Qwen2.5-VL 从这些文档中提取文本，并用 Qwen2.5 改进提取内容的质量。为了增加数学和代码数据的数量，我们利用 Qwen2.5-Math 和 Qwen2.5-Coder 这两个数学和代码领域的专家模型合成数据，合成了包括教科书、问答对以及代码片段等多种形式的数据。

### Qwen3模型采用三阶段预训练过程：

__第一阶段：基本语言知识和通用能力训练。__ 所有Qwen3模型使用4,096 token的序列长度，在超过30万亿token的数据上进行训练 。此阶段旨在建立模型的语言能力和通用世界知识基础，训练数据覆盖119种语言和方言 。

__第二阶段：复杂推理能力训练。__ 此阶段的模型使用4,096 token的序列长度，在约5万亿高质量token上进行进一步预训练，预训练数据包括STEM、编码、推理、数学和合成数据（知识密集型数据）等 。在此阶段还加速了学习率衰减 。

__第三阶段：长上下文能力训练。__ 此阶段将模型4K的输入长度扩展到32K，高质量长上下文语料中，75%的文本长度在16,384到32,768 token之间，25%的文本长度在4,096到16,384 token之间 。报告提及沿用Qwen2.5的做法，使用ABF技术将RoPE的基础频率从10,000提高到1,000,000 。同时，引入YARN和Dual Chunk Attention (DCA)技术，在推理过程中实现序列长度容量的四倍增长 。

## 后训练（Post-training）：四阶段能力精炼，实现逐步推理和快速响应

![image](https://github.com/user-attachments/assets/68712ee8-6235-4390-a8da-e493df905d5c)

为了开发能够同时具备思考推理和快速响应能力的混合模型，我们实施了一个四阶段的训练流程。该流程包括：（1）长思维链冷启动，（2）长思维链强化学习，（3）思维模式融合，以及（4）通用强化学习。

__第一阶段：长思维链冷启动。__ 使用多样的长思维链数据对模型进行微调（SFT），涵盖了数学、代码、逻辑推理和 STEM 问题等多种任务和领域。这一过程旨在为模型配备基本的推理能力。

__第二阶段：长思维链强化学习。__ 这一阶段的重点是大规模强化学习，利用基于规则的奖励来增强模型的探索和钻研能力，进一步提升模型的推理能力麻烦让模型能够更加有效的寻找最佳答案。

__第三阶段：思维模式融合。__ 我们在一份包括长思维链数据和常用的指令微调数据的组合数据上对模型进行微调（SFT），将非思考模式整合到思考模型中。确保了推理和快速响应能力的无缝结合。

__第四阶段：通用强化学习。__ 我们在包括指令遵循、格式遵循和 Agent 能力等在内的 20 多个通用领域的任务上应用了强化学习，以进一步增强模型的通用能力并纠正不良行为。

