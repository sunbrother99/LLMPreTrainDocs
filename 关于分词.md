# BPE、BBPE、subword粒度切词

根据不同的切分粒度可以把tokenizer分为: 基于词的切分，基于字的切分和基于subword的切分。 基于subword的切分是目前的主流切分方式。

subword的切分包括: BPE(/BBPE), WordPiece 和 Unigram三种分词模型。其中WordPiece可以认为是一种特殊的BPE。

完整的分词流程包括：文本归一化，预切分，基于分词模型的切分，后处理。

SentencePiece是一个分词工具，内置BEP等多种分词方法，基于Unicode编码并且将空格视为特殊的token。这是当前大模型的主流分词方案。

————————————————

BPE：GPT, Baichuan, RoBERTa,BART,DeBERTa

BBPE:ChatGLM

BPE / BBPE: GPT-2, GPT-4O, GPT3, Qwen, Qwen2, Qwen3, Llama, Llama2, Llama3

WordPiece：BERT, DistilBERT

Unigram：AlBERT, T5, mBART, XLNet

## 基于subword的切分

基于词和字的切分都会存在一定的问题，直接应用的效果比较差。

基于词的切分，会造成:

词表规模过大
一定会存在UNK，造成信息丢失
不能学习到词缀之间的关系，例如：dog与dogs，happy与unhappy

基于字的切分，会造成:

每个token的信息密度低
序列过长，解码效率很低

所以基于词和基于字的切分方式是两个极端，其优缺点也是互补的。而折中的subword就是一种相对平衡的方案。

基于subword的切分能很好平衡基于词切分和基于字切分的优缺点，也是目前主流最主流的切分方式。
subword的基本切分原则是：

高频词依旧切分成完整的整词
低频词被切分成有意义的子词，例如 dogs => [dog, ##s]

基于subword的切分可以实现：

词表规模适中，解码效率较高
不存在UNK，信息不丢失
能学习到词缀之间的关系

基于subword的切分包括：BPE，WordPiece 和 Unigram 三种分词模型。

### 处理流程概述

归一化:最基础的文本清洗，包括删除多余的换行和空格，转小写，移除音调等。

HuggingFace tokenizer的实现： https://huggingface.co/docs/tokenizers/api/normalizers

预分词:即分词，把句子切分成更小的“词”单元。可以基于空格或者标点进行切分。 不同的tokenizer的实现细节不一样。例如:

pre-tokenize:

[BERT]: [(‘Hello’, (0, 5)), (‘,’, (5, 6)), (‘how’, (7, 10)), (‘are’, (11, 14)), (‘you’, (16, 19)), (‘?’, (19, 20))]

[GPT2]: [(‘Hello’, (0, 5)), (‘,’, (5, 6)), (‘Ġhow’, (6, 10)), (‘Ġare’, (10, 14)), (‘Ġ’, (14, 15)), (‘Ġyou’, (15, 19)), (‘?’, (19, 20))]

[t5]: [(‘▁Hello,’, (0, 6)), (‘▁how’, (7, 10)), (‘▁are’, (11, 14)), (‘▁you?’, (16, 20))]

可以看到BERT的tokenizer就是直接基于空格和标点进行切分。

GPT2也是基于空格和标签，但是空格会保留成特殊字符“Ġ”。

T5则只基于空格进行切分，标点不会切分。并且空格会保留成特殊字符"▁"，并且句子开头也会添加特殊字符"▁"。

预分词的实现： https://huggingface.co/docs/tokenizers/api/pre-tokenizers

基于分词模型的切分:不同分词模型具体的切分方式。分词模型包括：BPE，WordPiece 和 Unigram 三种分词模型。

分词模型的实现： https://huggingface.co/docs/tokenizers/api/models

后处理:后处理阶段会包括一些特殊的分词逻辑，例如添加sepcial token：[CLS],[SEP]等。

后处理的实现： https://huggingface.co/docs/tok

--
huggingface中tokenizer的流程：

输入字符串 → Normalizer（标准化） → PreTokenizer（预切词） → Model (BPE/WordPiece/SentencePiece) → PostProcessor（加入cls,sep等特殊token） → token ID

1、Normalizer：规范化文本
用于处理统一的字符表示，例如：

Unicode NFKC 规范化（形态统一）

小写化（lowercase）

去除空白字符、替换特殊字符等

2、PreTokenizer：切分为初始 token

 这一步先把文本切成“预 token 单位”（不是最终 token），例如：
 按空格、标点分词（Whitespace, Punctuation）
 加空格标记：RoBERTa 使用 ByteLevel 分词器，会用 Ġ 或 ▁ 标记空格起始

3. Model：编码为 token（subword）

   这是核心模块，执行子词合并与 ID 映射，主流算法包括：

  ✅ BPE（Byte Pair Encoding）：
  初始是 byte
  
  按 merges.txt 中频率高的 pair 合并
  
  得到 subword → id 对照
  
  ✅ WordPiece：
  类似 BPE，但不是贪心合并，而是最大匹配（greedy longest match）
  
  使用特殊符号如 ## 表示连接
  
  ✅ Unigram（SentencePiece）：
  用概率模型挑选子词片段集合
  
  Google 的 T5、ALBERT 使用

  4. PostProcessor：添加特殊 token 等后处理
  用于添加 <s>, </s>, [CLS], [SEP] 等特殊 token。
