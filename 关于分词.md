# BPE、BBPE、subword粒度切词

根据不同的切分粒度可以把tokenizer分为: 基于词的切分，基于字的切分和基于subword的切分。 基于subword的切分是目前的主流切分方式。

subword的切分包括: BPE(/BBPE), WordPiece 和 Unigram三种分词模型。其中WordPiece可以认为是一种特殊的BPE。

完整的分词流程包括：文本归一化，预切分，基于分词模型的切分，后处理。

SentencePiece是一个分词工具，内置BEP等多种分词方法，基于Unicode编码并且将空格视为特殊的token。这是当前大模型的主流分词方案。

————————————————

BPE：GPT, Baichuan, RoBERTa,BART,DeBERTa

BBPE:ChatGLM

BPE / BBPE: GPT-2, GPT-4O, GPT3, Qwen, Qwen2, Qwen3, Llama, Llama2, Llama3

WordPiece：BERT, DistilBERT

Unigram：AlBERT, T5, mBART, XLNet

## 基于subword的切分

基于词和字的切分都会存在一定的问题，直接应用的效果比较差。

基于词的切分，会造成:

词表规模过大
一定会存在UNK，造成信息丢失
不能学习到词缀之间的关系，例如：dog与dogs，happy与unhappy

基于字的切分，会造成:

每个token的信息密度低
序列过长，解码效率很低

所以基于词和基于字的切分方式是两个极端，其优缺点也是互补的。而折中的subword就是一种相对平衡的方案。

基于subword的切分能很好平衡基于词切分和基于字切分的优缺点，也是目前主流最主流的切分方式。
subword的基本切分原则是：

高频词依旧切分成完整的整词
低频词被切分成有意义的子词，例如 dogs => [dog, ##s]

基于subword的切分可以实现：

词表规模适中，解码效率较高
不存在UNK，信息不丢失
能学习到词缀之间的关系

基于subword的切分包括：BPE，WordPiece 和 Unigram 三种分词模型。

### 处理流程概述

归一化:最基础的文本清洗，包括删除多余的换行和空格，转小写，移除音调等。

HuggingFace tokenizer的实现： https://huggingface.co/docs/tokenizers/api/normalizers

预分词:即分词，把句子切分成更小的“词”单元。可以基于空格或者标点进行切分。 不同的tokenizer的实现细节不一样。例如:

pre-tokenize:

[BERT]: [(‘Hello’, (0, 5)), (‘,’, (5, 6)), (‘how’, (7, 10)), (‘are’, (11, 14)), (‘you’, (16, 19)), (‘?’, (19, 20))]

[GPT2]: [(‘Hello’, (0, 5)), (‘,’, (5, 6)), (‘Ġhow’, (6, 10)), (‘Ġare’, (10, 14)), (‘Ġ’, (14, 15)), (‘Ġyou’, (15, 19)), (‘?’, (19, 20))]

[t5]: [(‘▁Hello,’, (0, 6)), (‘▁how’, (7, 10)), (‘▁are’, (11, 14)), (‘▁you?’, (16, 20))]

可以看到BERT的tokenizer就是直接基于空格和标点进行切分。

GPT2也是基于空格和标签，但是空格会保留成特殊字符“Ġ”。

T5则只基于空格进行切分，标点不会切分。并且空格会保留成特殊字符"▁"，并且句子开头也会添加特殊字符"▁"。

预分词的实现： https://huggingface.co/docs/tokenizers/api/pre-tokenizers

基于分词模型的切分:不同分词模型具体的切分方式。分词模型包括：BPE，WordPiece 和 Unigram 三种分词模型。

分词模型的实现： https://huggingface.co/docs/tokenizers/api/models

后处理:后处理阶段会包括一些特殊的分词逻辑，例如添加sepcial token：[CLS],[SEP]等。

后处理的实现： https://huggingface.co/docs/tok

--

### huggingface中Tokenizer的流程

输入字符串 → Normalizer（标准化） → PreTokenizer（预切词） → Model (BPE/WordPiece/SentencePiece) → PostProcessor（加入cls,sep等特殊token） → token ID

```text
1.Normalizer：规范化文本
用于处理统一的字符表示，例如：

Unicode NFKC 规范化（形态统一）
小写化（lowercase）
去除空白字符、替换特殊字符等

2.PreTokenizer：切分为初始 token

 这一步先把文本切成“预 token 单位”（不是最终 token），例如：
 按空格、标点分词（Whitespace, Punctuation）
 加空格标记：RoBERTa 使用 ByteLevel 分词器，会用 Ġ 或 ▁ 标记空格起始

3. Model：编码为 token（subword）

这是核心模块，执行子词合并与 ID 映射，主流算法包括：
  ✅ BPE（Byte Pair Encoding）：
  初始是 byte
  按 merges.txt 中频率高的 pair 合并
  得到 subword → id 对照
  
  ✅ WordPiece：
  类似 BPE，但不是贪心合并，而是最大匹配（greedy longest match）
  使用特殊符号如 ## 表示连接
  
  ✅ Unigram（SentencePiece）：
  用概率模型挑选子词片段集合
  Google 的 T5、ALBERT 使用

4. PostProcessor：添加特殊 token 等后处理
  用于添加 <s>, </s>, [CLS], [SEP] 等特殊 token。
```

## WordPiece与BPE的关系

这是个非常经典又容易混淆的问题，我们来系统解释：

---

#### 一句话总结：

> **WordPiece 和 BPE（Byte Pair Encoding）本质上是同一种思想的两种变体：**
> 它们都通过将**词拆成更小的子词（subword）**来解决词表爆炸和未登录词（OOV）问题，但在**训练方式和合并策略上略有不同**。

---

#### 相同点（共同理念）

| 方面          | 内容说明                             |
| ----------- | -------------------------------- |
| 🎯 目标       | 将词拆成子词，让模型能处理未见过的新词（如拼写错误、人名等）   |
| 🚫 不使用完整词表  | 不像传统 NLP 使用完整词作为 token，避免 OOV 问题 |
| 🧱 token 单位 | 基于字符、子词组合构建最终 token              |
| ✅ 自动学习子词    | 训练过程中基于数据自动学习高频组合                |

---

####  不同点：WordPiece vs. BPE

| 对比维度    | **WordPiece**                    | **BPE (Byte Pair Encoding)**      |
| ------- | -------------------------------- | --------------------------------- |
| 📚 起源   | Google（用于 BERT）                  | OpenAI GPT、RoBERTa 等广泛使用          |
| 🧠 核心思想 | 基于语言模型的最大似然估计选子词                 | 基于出现频率高的字符对逐步合并                   |
| 📈 训练目标 | 最小化 token 化后的语言模型困惑度（perplexity） | 最大化 token 合并后的词频提升                |
| 🔍 合并策略 | 找到最优的子词组合（信息量最大）                 | 每轮合并频率最高的 token pair              |
| 🧩 编码过程 | 贪心最大匹配（左到右）                      | 依次按 merge 表执行合并                   |
| 🔤 子词标记 | 使用 `##` 表示非首位子词                  | 无需特殊标记，但 ByteLevel BPE 用 `Ġ` 表示空格 |
| 💾 文件形式 | `vocab.txt`                      | `vocab.json` + `merges.txt`       |

---

#### 举个例子：词语 “unwantedly”

#### WordPiece 分词：

```
["un", "##want", "##ed", "##ly"]
```

→ 最大匹配（greedy longest match）

---

#### BPE 分词（GPT 风格）：

```
["un", "want", "ed", "ly"]
```

→ 频率驱动的 pair 合并（没有 `##`）

如果使用 RoBERTa 的 Byte-Level BPE，可能是：

```
["Ġun", "want", "ed", "ly"]
```

---

#### 技术上，WordPiece 可视为改进过的 BPE

* BPE 是“频率驱动合并”，比较“死板”
* WordPiece 引入了**语言建模目标函数**，用**条件概率优化子词选择**
* BPE 仅依赖频率统计，速度快但精度略低
* WordPiece 在处理**低频复合词**时更稳健，比如医学、命名实体等领域

---

#### 总结理解关系：

| 说法                         | 正确性                | 解释 |
| -------------------------- | ------------------ | -- |
| WordPiece 是 BPE 的一种        | ✅ 基于共同思想的变体，使用不同策略 |    |
| WordPiece 比 BPE 更复杂        | ✅ 使用语言模型优化         |    |
| WordPiece 完全不同于 BPE        | ❌ 二者核心思想是一致的       |    |
| BERT 用 WordPiece，GPT 用 BPE | ✅ 主流实现确实如此         |    |

---


## BERT / RoBERTa / ALBERT / Qwen 的 Tokenizer
---

## 模型分词器对比总览

| 模型          | 分词器类型           | 分词算法           | 基本单位        | 是否字节级 | 分词词表格式                    | 特点说明                |
| ----------- | --------------- | -------------- | ----------- | ----- | ------------------------- | ------------------- |
| **BERT**    | WordPiece       | 最大匹配子词（Greedy） | Unicode 字符串 | 否     | `vocab.txt`               | 需空格分词，适用于英文         |
| **RoBERTa** | Byte-Level BPE  | BPE            | UTF-8 字节    | 是     | `vocab.json + merges.txt` | 支持任意语言，字节级合并        |
| **ALBERT**  | SentencePiece   | Unigram 模型     | Unicode 字符串 | 否     | `.spm` 文件                 | 无需预分词，支持子词采样        |
| **Qwen**    | 自研 BBPE（字节 BPE） | Byte-BPE（变体）   | UTF-8 字节    | 是     | `vocab.json + merges.txt` | 真正字节级编码，适配多语言、emoji |

---

## 📌 各模型分词器说明详解

### **BERT**

* 使用 Google 的 WordPiece，适合英文。
* 基于最大子词匹配，无法处理未登录词时性能较差。
* 对中文支持需特殊训练（BERT-Chinese 版）。

### **RoBERTa**

* 使用 HuggingFace 的 ByteLevel BPE（GPT-2 风格）。
* 先将文本转为字节序列，再按 byte pair 合并规则生成 token。
* 更适合多语言、Unicode 和代码等输入。

### **ALBERT**

* 使用 Google 的 SentencePiece（`.spm` 文件）。
* 分词算法是 **Unigram Language Model**，通过概率建模选出最佳子词序列。
* 支持子词正则化（subword sampling），提升模型泛化能力。
* 无需手动空格分词，适配多语言。

### **Qwen**

* 使用 Qwen 自研的 BBPE（Byte Byte Pair Encoding）分词器。
* 从 UTF-8 字节级别建模并合并，避免 unicode 异常。
* 专为中文、emoji、代码混合输入优化。
* `vocab.json + merges.txt` 与 GPT-2 格式相似，但处理逻辑不同。

---

## 🔍 词表示单位对比示例（以“你好世界”为例）

| 模型      | 分词结果（Token）           | 备注                          |
| ------- | --------------------- | --------------------------- |
| BERT    | \['你', '好', '世', '界'] | 中文版本专用 vocab，按字拆分           |
| RoBERTa | \['▁你好', '世界']        | `▁` 表示空格开头，来自 ByteLevel BPE |
| ALBERT  | \['▁你好', '，', '世界']   | SentencePiece 输出，`▁` 表示词首   |
| Qwen    | \['你好', '世界']（合并字节）   | 实际是字节合并后 token，非表面文字        |

---

## ✅ 总结一句话：

> **ALBERT 使用 SentencePiece 的 Unigram 分词器，而不是 WordPiece。它支持概率建模和子词采样；相比之下，RoBERTa 和 Qwen 使用字节级 BPE，支持更灵活的输入；BERT 则使用传统的 WordPiece，只适合英文类场景。**

---



---
