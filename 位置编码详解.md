
### 1、语言模型为什么需要位置编码？

由于LLM的attention机制计算词向量相关性是位置无关的，所以导致词向量不知道上下文的位置，而只知道上下文是什么。

比如 “我妈想你了” 和 “我想你妈了”，虽然都是一样的词向量，但是因为词向量位置不一样，语义信息也完全不一样。所以需要给词向量加一个位置编码，告诉网络每一个词向量在句子中的位置。

### 2、位置编码有哪些？

绝对位置编码：像贴标签一样给每个位置固定编号，无法灵活处理长度变化。即直接将位置的表示加到token的表示上，而每个位置的表示则为一个可学习的向量。常见的Bert, Roberta，ALbert均使用了绝对位置编码，模型的固定最大长度是512，不能后续扩展，需要来截断输入文本，这样会影响长文本的效果。

绝对位置编码分为可学习绝对位置编码和固定绝对位置编码。bert系列都是可学习绝对位置编码，position embedding在预训练模型时随机初始化。最开始的Transformer（attention is all you need ）使用的是固定绝对位置编码（三角函数编码）

被称为learnable绝对位置编码，存在着两个问题：(1) 位置编码本身通过大量数据才能学习到；(2) 位置向量之间的相对关系没有被利用到，如位置1和位置2之间的相似性应比位置1和位置9之间的相似性高。

相对位置编码：需要预定义最大相对距离，长文本处理困难。
绝对位置编码是将位置编码直接嵌入到序列的表示中，而相对位置编码则是指在计算注意力分数的时候，直接考虑两个token之间的相对位置，即
<img width="345" alt="image" src="https://github.com/user-attachments/assets/de1a13b4-425f-45da-92fc-acf913978201" />

其中， b{i_j}被称为position_bias，根据其形式的确定，可以将相对位置编码大致分为两种流派：

<img width="461" alt="image" src="https://github.com/user-attachments/assets/918c1e23-411e-4283-ba3c-1ec5d7fe268c" />

使用相对位置编码的模型有： Transformer-XL，T5

可学习参数：存在过拟合风险，外推性差

#### 绝对位置 v.s. 相对位置

#### 绝对位置编码

绝对位置编码具有实施简单、计算速度快的优点，绝对位置编码的目标就是为输入序列中的每个位置生成一个唯一的向量。而其缺点也是明显的，因为真正重要的往往不是绝对位置，而是token之间的相对位置。在下面三个句子中，东西的含义和东西与鱼的相对位置有关，而与东西本身的绝对位置无关。

```text
有个东西在吃鱼
小明放眼望去，看到有个东西在吃鱼
有条鱼在吃东西
```

##### 正余弦编码

绝对位置编码最常见的就是正余弦编码，下面的公式中，d表示特征通道数，0<=i<d/2 表示channel 维度。

<img width="278" alt="image" src="https://github.com/user-attachments/assets/bab7ada6-ca68-4409-9f01-a4995919026c" />

不同位置（pos）的位置编码是根据频率编码的。由于正余弦函数的周期性，不同位置的位置编码也能感知到一定的相对距离。

虽然正余弦位置编码（又叫三角式位置编码），作为一种绝对位置编码，包含了一定相对位置信息，但这种相对位置信息仅仅包含在位置编码内部。当添加位置编码的表示在计算自注意力的时候，表示中的相对位置信息是否仍然保留就是个未知数了。

此外，对于线性attention而言，相对位置编码无法直接得到应用。因此，沿着三角式位置编码的思路，进一步发展绝对位置编码是有必要的。

##### 可学习编码

另一种常见的绝对位置编码是可学习编码，跟LLM等工作一样，最常规的操作是可以从torch 初始化一个nn.embedding R_{Nxd}
，N就是序列长度。当然，可学习就比较黑盒了，除此之外，笔者也见过也有拿能padding的conv作为位置编码，因为padding conv也能捕捉位置变化。

#### 相对位置编码

绝对位置编码直接表征哪个位置的向量有哪个编码，而相对位置编码是以 两个词向量 相对位置而构建的位置编码。

上面绝对位置编码的公式展开后如下，和位置相关的编码标记为红色

<img width="651" alt="image" src="https://github.com/user-attachments/assets/b9965be5-23e5-4f8d-b928-a84d76cd9e6c" />

##### T5实现

T5的实现用的是想多位置编码，它把后面一系列位置编码都浓缩成了一个相对位置编码$b_ij$，$b_ij$只和i,j的相对位置有关。

A = softmax(x_iW{q}W_kx_j+B_ij) = softmax(qk+b_ij)

##### RoPE旋转位置编码

旋转位置编码，也就是现在llama、qwen等LLM、VLM最喜欢用的一种位置编码形式。其核心是约束位置m的query embedding $q_m$和位置n的key embedding $k_n$的点积，只和$q_m$、$k_n$、相对位置（m-n）相关。

<img width="380" alt="image" src="https://github.com/user-attachments/assets/148f524a-1286-48f1-8e52-79809f6a8fc9" />

为了凑得这样的等式，这里苏神采用的方式是映射到复数空间。注意这里多出了一个变量θ,为啥又引入了一个变量 θ我们之后讨论，先继续看，不影响理解。

<img width="355" alt="image" src="https://github.com/user-attachments/assets/68e733e1-c839-4542-ace9-630a2321e55d" />

相乘后，实部就只取决于$q_m$,$k_n$,而位置（m-n）只影响虚部。

<img width="626" alt="image" src="https://github.com/user-attachments/assets/7d554719-5609-4fc3-ad8a-5576450148cd" />

$ k_n$表示$ k_n$的共轭复数，上面的形式是一个点到点（m位置和n位置的点积），实现的时候肯定要矩阵化啊，矩阵计算是batch推理的基础。

矩阵化之前，我们需要再拆解一下$f_q(q_m,m)$。

首先，<img width="243" alt="image" src="https://github.com/user-attachments/assets/95fd191a-54dc-422b-9a03-b645bb224939" />是复平面上单位圆的表达式，模长为1，$q_m$也可以拆解为实部和虚部
<img width="152" alt="image" src="https://github.com/user-attachments/assets/677024ec-2d1a-487f-8250-3c280c2b5016" />，展开得到：
<img width="640" alt="image" src="https://github.com/user-attachments/assets/aa5583ee-50d4-46e3-bb37-0a22a4acf37b" />

首先需要向量化，实部和虚部就是两个不同的维度。
<img width="659" alt="image" src="https://github.com/user-attachments/assets/a07ba7b8-e187-4d35-b57a-4c96d17653e2" />

我们看看物理意义是啥，快看，前面的2x2矩阵 不就是一个旋转矩阵吗？后面2x1是一个实部和虚部的幅值，也就是两个不同维度的轴（比如我们常说的xy轴，正交）。这不就是对两个轴做了线性变换吗，造成了坐标轴中的向量发生“旋转”。这就是“旋转位置编码”名字的由来。

同理可得
<img width="380" alt="image" src="https://github.com/user-attachments/assets/3c4f65c3-b130-43f6-827a-f47484c032ed" />

我们把qk乘到一起，中间就是m-n的旋转矩阵。

<img width="671" alt="image" src="https://github.com/user-attachments/assets/ffee31bd-0d23-4699-ba08-04f1d2373a09" />

最后一步，对于实际要学习的query，q是一个Rxd的高位响亮，我们上面推到的是点到点两个轴（m位置和n位置的点积），如何运用到实际学习的q矩阵上去呢？

我们可以近似把通道轴堪称C个独立的轴，哪呢就可以相邻的轴两两凑对了。举个例子，1024维度就可以分为512个对，进而没对可以充当公式中的实部和虚部，同时，假设第i对的角度差是$θ{i}$,则可以得到下面的旋转矩阵。

<img width="678" alt="image" src="https://github.com/user-attachments/assets/caa14f11-7331-43f9-935f-341c20edef4a" />

我们知道正余弦函数的周期是2π,如果只是用相对位置m、n，那么很快就溢出一个周期了，这会导致相位的重复或重叠，影响模型对不同位置的区分能力，实际上rope的$θ{i}$采用的和正余弦编码类似的形式。

<img width="330" alt="image" src="https://github.com/user-attachments/assets/fe524b13-9a7b-4e7c-a536-ab36670e5e5f" />

当θ是个极小值时，mθ是个小于2π的值，这样相对位置就不会超过2π的周期了。底数10000是个经验值。


### 3、理想的位置编码需要有什么特性？

1、每个位置的词向量的位置编码唯一
2、每个词向量之间的位置编码具有线性关系，比如1到3，100到102
3、可以泛化到训练没见过的更长的序列
4、维度的可扩展性（1d,2d,3d等），需要原声支持更高维度



