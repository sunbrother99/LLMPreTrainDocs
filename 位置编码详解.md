
### 1、语言模型为什么需要位置编码？

当你在读这句话时，大脑不仅能识别每个字的含义，还能理解它们的先后顺序。但对于Transformer模型来说，它最初设计的自注意力机制（self-attention）就像一个"脸盲症患者"——虽然能清楚每个字的语义信息，却无法分辨它们出现的先后顺序。 因此，我们需要位置编码（Positional Encoding），给每个字打上"座位号"，让模型知道谁先谁后。

### 2、位置编码有哪些？

绝对位置编码：像贴标签一样给每个位置固定编号，无法灵活处理长度变化。即直接将位置的表示加到token的表示上，而每个位置的表示则为一个可学习的向量。常见的Bert, Roberta，ALbert均使用了绝对位置编码，模型的固定最大长度是512，不能后续扩展，需要来截断输入文本，这样会影响长文本的效果。

绝对位置编码分为可学习绝对位置编码和固定绝对位置编码。bert系列都是可学习绝对位置编码，position embedding在预训练模型时随机初始化。最开始的Transformer（attention is all you need ）使用的是固定绝对位置编码（三角函数编码）

被称为learnable绝对位置编码，存在着两个问题：(1) 位置编码本身通过大量数据才能学习到；(2) 位置向量之间的相对关系没有被利用到，如位置1和位置2之间的相似性应比位置1和位置9之间的相似性高。

相对位置编码：需要预定义最大相对距离，长文本处理困难。
绝对位置编码是将位置编码直接嵌入到序列的表示中，而相对位置编码则是指在计算注意力分数的时候，直接考虑两个token之间的相对位置，即
<img width="345" alt="image" src="https://github.com/user-attachments/assets/de1a13b4-425f-45da-92fc-acf913978201" />

其中， b{i_j}被称为position_bias，根据其形式的确定，可以将相对位置编码大致分为两种流派：

<img width="461" alt="image" src="https://github.com/user-attachments/assets/918c1e23-411e-4283-ba3c-1ec5d7fe268c" />

使用相对位置编码的模型有： Transformer-XL，T5

可学习参数：存在过拟合风险，外推性差

绝对位置 v.s. 相对位置

绝对位置编码具有实施简单、计算速度快的优点。而其缺点也是明显的，因为真正重要的往往不是绝对位置，而是token之间的相对位置。在下面三个句子中，东西的含义和东西与鱼的相对位置有关，而与东西本身的绝对位置无关。

```text
有个东西在吃鱼
小明放眼望去，看到有个东西在吃鱼
有条鱼在吃东西
```

虽然三角式位置编码，作为一种绝对位置编码，包含了一定相对位置信息，但这种相对位置信息仅仅包含在位置编码内部。当添加位置编码的表示在计算自注意力的时候，表示中的相对位置信息是否仍然保留就是个未知数了。

此外，对于线性attention而言，相对位置编码无法直接得到应用。因此，沿着三角式位置编码的思路，进一步发展绝对位置编码是有必要的。
